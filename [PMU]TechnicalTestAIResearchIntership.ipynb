{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thad75/-PMUTest-/blob/main/%5BPMU%5DTechnicalTestAIResearchIntership.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY1zWsxO6UnD"
      },
      "source": [
        "# Test Technique Stage Recherche PMU\n",
        "\n",
        "Welcome to Research AI Technical Test.\n",
        "\n",
        "**Process**:\n",
        "*   Take 20-30 min to do as much as you can.\n",
        "*   Send back the test at tharsan.senthivel.cyu@ext.pmu.fr\n",
        "*   10 min interview to discuss about the test.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7VkXorwTgq6"
      },
      "source": [
        "**The goal of this test** :\n",
        "\n",
        "*   Direct application of DL Course\n",
        "*   Use Pytorch to create and train a Neural Network\n",
        "\n",
        "The tutorial is composed of one main part :  \n",
        "* a Warmup on PyTorch\n",
        "\n",
        "\n",
        "Your best friend will be the Pytorch Documentation:\n",
        "\n",
        "- https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "- https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
        "\n",
        "We will look at your reasoning and coding skills.\n",
        "\n",
        "\n",
        "*   There are #TODO to fill up within Code Cells\n",
        "*   There are Questions in Markdown Cells\n",
        "\n",
        "---\n",
        "\n",
        "**Please do not spend more than 30 min on this test**\n",
        "\n",
        "---\n",
        "\n",
        "Feel free to contact me if you have issues opening the test.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLqCCALt1ONt"
      },
      "source": [
        "Please use Google Colab available on Google Drive as the libraries are already installed.\n",
        "Don't forget to activate your GPU by going to :\n",
        "Exécution -> Modifier le type d'éxecution -> GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR-xX5bcHd8M"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-L2tscp1ObV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchmetrics import Accuracy\n",
        "from tqdm import tqdm\n",
        "\n",
        "torch.manual_seed(42)\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJdIaGITzHRP"
      },
      "source": [
        "# A Simple Neuron\n",
        "\n",
        "Let's introduce a Simple Task.\n",
        "Given a boolean value (0 or 1), we want the model to invert the input value.\n",
        "Example : if the model's input is 0, we want it to output 1.\n",
        "\n",
        "In order to perform this task, we will need few elements:\n",
        "* a Dataset\n",
        "* a Model\n",
        "* a Training/Testing Loop\n",
        "* Some hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8os86fyOmGQ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Question : What task are we performing ? Is it regression or classification?\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nh8xJZ0EWhG"
      },
      "source": [
        "## a - The Simple Dataset and Dataloader\n",
        "\n",
        "As you know, to train a model you will need data. In practice before choosing/creating a model, we usually have a look on the Dataset. These datas come in the form of labeled or unlabeled data.\n",
        "\n",
        "In Pytorch, datasets inherits from the Dataset class. It is a simple class composed of minimum 3 methods :\n",
        "* __init__ : to initialize the class\n",
        "* __getitem__ : to retrieve a sample according to a index number\n",
        "* __len__ : to return the len of the entire Dataset\n",
        "\n",
        "In our case, we will generate a list of 0 and 1. The __getitem__ method should return the opposite value of the element picked at the given index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgwLl3CvzxGh"
      },
      "outputs": [],
      "source": [
        "class SimpleDataset(Dataset):\n",
        "\n",
        "  def __init__(self, len_data):\n",
        "    self.len_data = len_data\n",
        "    self.data = [random.randint(0,1) for i in range(len_data)] # This list of length len_data is filled with 0 and 1\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    # We select as data the value at index idx of the self.data list\n",
        "    data = self.data[idx]\n",
        "    label = ... if self.data[idx]== 0 else ...\n",
        "    # TODO : Return a dictionnary {'data': .., 'label':}\n",
        "    return {\"data\":torch.as_tensor(),\n",
        "            \"label\": torch.as_tensor()}\n",
        "\n",
        "  def __len__(self):\n",
        "    # Explanation : We know that the total length of the dataset is the length of the self.data attribute\n",
        "    return len(self.data)\n",
        "\n",
        "# TODO : Create dataset_train and dataset_test by initializing the Classes. You can choose a big value for the size of the list\n",
        "len_dataset_train = len_dataset_test= ...\n",
        "dataset_train = SimpleDataset(len_dataset_train)\n",
        "dataset_test = SimpleDataset(len_dataset_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLToTyilGSu5"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Question: What is the difference between a DataLoader and a Dataset ?\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bx2poGf2Dw2"
      },
      "outputs": [],
      "source": [
        "# TODO : Create the DataLoaders\n",
        "\n",
        "dataset_train = DataLoader(dataset_train, batch_size= 2048)\n",
        "dataset_test = DataLoader(dataset_test, batch_size= 2048)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiwHPWo8GdKY"
      },
      "source": [
        "## b - The Simple Model\n",
        "\n",
        "First time coding a neural net ? Let's think a little bit.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Questions :\n",
        "*  **Is one neuron enough to perform the inversion of a boolean ?**\n",
        "* **Given an input x, a weight w, a bias b and an activation function f, how do we modelize a Single Neuron ?**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44xzhLcRHCru"
      },
      "source": [
        "Creating a Model in PyTorch is pretty simple. We initalize a Class that inherits from nn.Module.\n",
        "\n",
        "It is defined as follows:\n",
        "\n",
        "\n",
        "```\n",
        "class Model(nn.Module):\n",
        "  def __init__(self,...):\n",
        "    \"\"\"\"\n",
        "    Defines the model. You can put the input size as a parameter if needed..\n",
        "    \"\"\"\"\n",
        "    super().__init__() #to init the main class\n",
        "    self.layers = ... # defining the model : could be Conv2d, Linear, RNN, LSTM\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    \"\"\"\n",
        "    The input x is forwarded through the neural net.\n",
        "    \"\"\"\n",
        "    output = self.layers(x)\n",
        "    return output\n",
        "```\n",
        "\n",
        "More informations : https://pytorch.org/docs/stable/nn.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5M_a6_VdOp1"
      },
      "source": [
        "**Using a nn.Parameter, create a single Neuron model.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdburjUE1Ktu"
      },
      "outputs": [],
      "source": [
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self,):\n",
        "      super().__init__()\n",
        "      # Note : We initialize randomly the valuees of the weights and the biases as these are learned through the training\n",
        "      self.w = nn.Parameter(torch.tensor(random.random()), requires_grad =True) # Weight\n",
        "      # TODO : by looking at the weight parameter (self.w) initialize the bias\n",
        "      self.b =  # Bias\n",
        "      # TODO : add a Sigmoid Activation layer\n",
        "      self.sigmoid =\n",
        "\n",
        "    def forward(self,x):\n",
        "      # TODO : Using the famous formula of a single neuron, compute the value of x1 : the output of the neuron\n",
        "      x1 =\n",
        "      # TODO : Pass x1 through the activation layer\n",
        "      x1 =\n",
        "      return x1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtBBFyV-2ebo",
        "outputId": "95bb99c8-6b3b-4a9c-f0ff-818602ff0de0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SimpleModel(\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# TODO : Create an instance of the SimpleModel and print the layers it has using print()\n",
        "model =\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU-01GdZIZN3"
      },
      "source": [
        "## c- The Simple Training\n",
        "\n",
        "As of now, we are supposed to have the model and the dataset. Hence, we need to create the training loop and initialize some useful object for the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw1xLpjqIeDF"
      },
      "source": [
        "\n",
        "The training loop is defined as follows :\n",
        "\n",
        "\n",
        "```\n",
        "for epoch in num_epoch :\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):        \n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i == ??:\n",
        "          running_loss= running_loss/??\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSXJ5iD1I7_9"
      },
      "source": [
        "As you can see, there are some obscure words that we didn't define yet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bwijU5hJDm4"
      },
      "source": [
        "### i - Optimizer\n",
        "\n",
        "Optimizers are algorithms or methods used to minimize an error function (loss function) or to maximize the efficiency of production. A learning rate must be defined. The learning rate handles the step of the Gradient Descent update. Here, we will use stochastic gradient descent (SGD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGSEliitJap_"
      },
      "outputs": [],
      "source": [
        "# TODO : Fill the missing stuffs\n",
        "optimizer = optim.SGD(model.parameters(), lr=...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLMiRb1cJI-_"
      },
      "source": [
        "### ii - Criterion\n",
        "\n",
        "This is basically the loss function. It measures the differences between the output and the input. We want to minimize it.\n",
        "\n",
        "\n",
        "**Which Loss function will we be using in this Binary Classification Task ?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDYAyh-sKDr7"
      },
      "outputs": [],
      "source": [
        "# TODO : Initialize a criterion. Choose between BCELoss or MSELoss. More Information in https://pytorch.org/docs/stable/nn.html#loss-functions\n",
        "criterion ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64JbigwpJMmI"
      },
      "source": [
        "### iii - Num Epoch\n",
        "\n",
        "The number of epoch corresponds to the number of time, the model will see the samples from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUiLjFurKOuf"
      },
      "outputs": [],
      "source": [
        "# TODO : Choose a number of epochs\n",
        "num_epochs ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOy1JTJ_JSMF"
      },
      "source": [
        "### iv - Device\n",
        "\n",
        "Once the model initialized, we prefer accelerating the training. Hence, we send it to the GPU to profit from the parralelisation property of GPUs. To send it you have to define a device as follows:\n",
        "\n",
        "```\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "```\n",
        "To send your model or data to device :\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "model = model.to(device) or data = data.to(device)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yz0buH5KejY"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXzsX8KsK8aY"
      },
      "source": [
        "### v - Gathering everything under a Loop\n",
        "\n",
        "Let's gather everything under a Training loop and a Testing loop.\n",
        "* The training loop will update the weights of the model\n",
        "* The testing loop will only test the model without updating anything.\n",
        "\n",
        "\n",
        "**Fill in the blanks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoRHPfMLK_QJ"
      },
      "outputs": [],
      "source": [
        "# Defining the Training Loop :\n",
        "# Define Device :\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# TODO : Create an Instance of the Model\n",
        "model = ....to(device)\n",
        "\n",
        "# Define your device :\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# TODO : Define your optimizer. Don't forget your learning rate :\n",
        "lr = ...\n",
        "optimizer = ...\n",
        "\n",
        "# TODO : Define your criterion :\n",
        "criterion = ...\n",
        "\n",
        "# TODO : Define your number of epochs :\n",
        "n_epoch = ...\n",
        "\n",
        "# Initializing some stuff for visualization\n",
        "loss_train, loss_test = [], []\n",
        "\n",
        "\n",
        "for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataset_train, 0):\n",
        "        # TODO : Get the inputs. data is a dictionnary with keys : data and label\n",
        "        inputs, labels = ... .to(device), ... .to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # TODO : Send the input through the model\n",
        "        outputs = model(...)\n",
        "\n",
        "        # TODO : Compute the loss between the ouputs and the targets\n",
        "        loss = criterion(..., labels.float())\n",
        "\n",
        "        # update the weights using backward and optimizer\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 19:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss train: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "    loss_train.append(running_loss/i)\n",
        "\n",
        "    # Explanation : We don't need gradients here as we are not updating anything. This we call torch.no_grad()\n",
        "    with torch.no_grad():\n",
        "      for i, data in enumerate(dataset_test, 0):\n",
        "        # TODO : Get the inputs. data is a dictionnary with keys : data and label\n",
        "        inputs, labels =....to(device), ...\n",
        "\n",
        "        # TODO : Send the input through the model\n",
        "        outputs = model(...)\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 19:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss test: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "    loss_test.append(running_loss/i)\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLbnpF_wOc0y"
      },
      "source": [
        "### vi - Let's visualize some stuff\n",
        "\n",
        "As you could see we have to lists where we logged loss values each epochs. Let's plot them to see if the model is overfitting/underfitting.\n",
        "* **What can you tell on your model's training ?**\n",
        "* **Can we actually tell if the model is overfitting or not ? Why ?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDfHmMkeMkAu"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.title('Loss')\n",
        "plt.plot(...)\n",
        "plt.plot(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_wcP0PmO7ka"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}